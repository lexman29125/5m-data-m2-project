# Building dbt to Transform raw data into Data Warehouse

Set up a dbt project inside your current folder (5m-data-m2-project), with sources pointing at your raw CSVs under assets/, and models to build your star schema (fact + dimensions).



## 1. Init dbt Project

From inside 5m-data-m2-project, run:

dbt init dbt_olist

This creates a dbt_olist/ folder with starter configs.
Since you already have an assets/ folder with CSVs, weâ€™ll configure dbt to treat them as seeds or sources.



## 2. Project Structure

Weâ€™ll use:
```
5m-data-m2-project/
â”œâ”€â”€ assets/                       # your CSVs
â”œâ”€â”€ dbt_olist/                    # new dbt project
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_order_items.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_order_payments.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_products.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_sellers.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_order_reviews.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ dim_customers.sql
â”‚   â”‚       â”œâ”€â”€ dim_products.sql
â”‚   â”‚       â”œâ”€â”€ dim_sellers.sql
â”‚   â”‚       â”œâ”€â”€ dim_dates.sql
â”‚   â”‚       â”œâ”€â”€ dim_payments.sql
â”‚   â”‚       â””â”€â”€ fact_order_items.sql
â”‚   â””â”€â”€ seeds/
â”‚       â””â”€â”€ product_category_name_translation.csv

```


## 3. dbt_project.yml

Inside dbt_olist/dbt_project.yml:
```
name: 'dbt_olist'
version: '1.0.0'
profile: 'dbt_olist'

model-paths: ["models"]
seed-paths: ["seeds"]

models:
  dbt_olist:
    staging:
      materialized: view
    marts:
      materialized: table
```

To materialize dimensions as tables and fact as incremental (so you can rerun dbt without reloading the full dataset each time)? Thatâ€™s common in BigQuery setups to save cost.
```
name: 'dbt_olist'
version: '1.0.0'
profile: 'dbt_olist'

model-paths: ["models"]
seed-paths: ["seeds"]

models:
  dbt_olist:
    staging:
      materialized: view    # staging = lightweight views
    marts:
      dim_customers:
        materialized: table
      dim_products:
        materialized: table
      dim_sellers:
        materialized: table
      dim_dates:
        materialized: table
      dim_payments:
        materialized: table
      fact_order_items:
        materialized: incremental
        incremental_strategy: merge
        unique_key: order_id
```

## 4. Profiles

Running with BigQuery, weâ€™ll configure profiles.yml so that:

	â€¢	Source schema = m2_ingestion (your Meltano-loaded raw CSVs).
    â€¢	Target schema = m2_prod (your transformed star schema).

In ~/.dbt/profiles.yml, create a profile ():
```
dbt_olist:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account  # or oauth if using gcloud auth
      project: sound-vehicle-468314-q4   # <-- your GCP project ID
      dataset: m2_prod                  # <-- dbt will build models here
      threads: 4
      timeout_seconds: 300
      location: US
      keyfile: /path/to/your/service_account.json  # if using service account
```


## 5. Sources

Create models/staging/sources.yml:

```
version: 2

sources:
  - name: raw
    schema: m2_ingestion     # raw staging area in BigQuery
    tables:
      - name: olist_customers_dataset
      - name: olist_orders_dataset
      - name: olist_order_items_dataset
      - name: olist_order_payments_dataset
      - name: olist_products_dataset
      - name: olist_sellers_dataset
      - name: olist_order_reviews_dataset
```



## 6. Staging Models
Letâ€™s build the full dbt SQL codebase based on the project structure:

	â€¢	Sources â†’ Staging â†’ Dimensions & Fact
	â€¢	Sources are your 7 CSV tables in m2_ingestion
	â€¢	Targets are the star schema tables in m2_prod

ðŸ”¹ a. sources.yml (staging config)

models/staging/sources.yml
```
version: 2

sources:
  - name: raw
    schema: m2_ingestion
    tables:
      - name: olist_customers_dataset
      - name: olist_orders_dataset
      - name: olist_order_items_dataset
      - name: olist_order_payments_dataset
      - name: olist_products_dataset
      - name: olist_sellers_dataset
      - name: olist_order_reviews_dataset
```

ðŸ”¹ b. Staging Models

Each staging file just selects and cleans columns.

stg_customers.sql
```
with source as (
    select * from {{ source('raw', 'olist_customers_dataset') }}
)
select
    customer_id,
    customer_unique_id,
    customer_city,
    customer_state
from source
```

stg_orders.sql
```
with source as (
    select * from {{ source('raw', 'olist_orders_dataset') }}
)
select
    order_id,
    customer_id,
    cast(order_purchase_timestamp as date) as order_date,
    cast(order_delivered_customer_date as date) as delivered_date
from source
```

stg_order_items.sql
```
with source as (
    select * from {{ source('raw', 'olist_order_items_dataset') }}
)
select
    order_id,
    order_item_id,
    product_id,
    seller_id,
    price,
    freight_value
from source
```

stg_order_payments.sql
```
with source as (
    select * from {{ source('raw', 'olist_order_payments_dataset') }}
)
select
    order_id,
    payment_type,
    payment_value
from source
```

stg_products.sql
```
with source as (
    select * from {{ source('raw', 'olist_products_dataset') }}
)
select
    product_id,
    product_category_name,
    product_weight_g,
    product_length_cm,
    product_height_cm,
    product_width_cm
from source
```

stg_sellers.sql
```
with source as (
    select * from {{ source('raw', 'olist_sellers_dataset') }}
)
select
    seller_id,
    seller_city,
    seller_state
from source
```

stg_order_reviews.sql
```
with source as (
    select * from {{ source('raw', 'olist_order_reviews_dataset') }}
)
select
    order_id,
    review_score
from source
```

ðŸ”¹ c. Dimension Models

dim_customers.sql
```
{{ config(materialized='table') }}

select distinct
    customer_id,
    customer_unique_id,
    customer_city,
    customer_state
from {{ ref('stg_customers') }}
```

dim_products.sql
```
{{ config(materialized='table') }}

select
    p.product_id,
    t.product_category_name_english,
    p.product_weight_g,
    p.product_length_cm,
    p.product_height_cm,
    p.product_width_cm
from {{ ref('stg_products') }} p
left join {{ ref('product_category_name_translation') }} t
    on p.product_category_name = t.product_category_name
```

dim_sellers.sql
```
{{ config(materialized='table') }}

select distinct
    seller_id,
    seller_city,
    seller_state
from {{ ref('stg_sellers') }}
```

dim_dates.sql
```
{{ config(materialized='table') }}

with dates as (
    select distinct order_date as full_date
    from {{ ref('stg_orders') }}
    where order_date is not null
)
select
    cast(format_date('%Y%m%d', full_date) as int) as date_key,
    full_date,
    format_date('%A', full_date) as day_of_week,
    format_date('%B', full_date) as month_name,
    extract(year from full_date) as year,
    extract(quarter from full_date) as quarter
from dates
```

dim_payments.sql
```
{{ config(materialized='table') }}

select distinct
    payment_type as payment_type_key,
    payment_type
from {{ ref('stg_order_payments') }}
```

ðŸ”¹ d. Fact Model

fact_order_items.sql
```
{{ config(
    materialized='incremental',
    unique_key='order_id',
    incremental_strategy='merge'
) }}

with oi as (
    select * from {{ ref('stg_order_items') }}
),
o as (
    select * from {{ ref('stg_orders') }}
),
p as (
    select * from {{ ref('stg_order_payments') }}
),
r as (
    select * from {{ ref('stg_order_reviews') }}
)

select
    oi.order_id,
    oi.order_item_id,
    oi.product_id,
    oi.seller_id,
    o.customer_id,
    cast(format_date('%Y%m%d', o.order_date) as int) as order_date_key,
    p.payment_type as payment_type_key,
    oi.price,
    oi.freight_value,
    r.review_score,
    date_diff(o.delivered_date, o.order_date, day) as delivery_time_days
from oi
join o on oi.order_id = o.order_id
left join p on oi.order_id = p.order_id
left join r on oi.order_id = r.order_id

{% if is_incremental() %}
  -- Only load new/updated orders
  where o.order_date > (select max(full_date) from {{ ref('dim_dates') }})
{% endif %}
```

ðŸ”¹ e. Seed

Copy product_category_name_translation.csv into:
```
dbt_olist/seeds/product_category_name_translation.csv
```
Run it with:
```
dbt seed
```

ðŸ”¹ f. Workflow
	i.	Test connection:
```
dbt debug
```

	ii.	Load seed:
```
dbt seed
```

	iii.	Build staging + marts:
```
dbt run
```

âœ… Thatâ€™s the full dbt SQL codebase:
	â€¢	Staging = views
	â€¢	Dimensions = tables
	â€¢	Fact = incremental fact table

## 7. dbt Tests 

schema.yml for primary keys (unique, not null) on customer_id, product_id, seller_id, and order_id so you can validate data quality every run.

Letâ€™s add dbt tests for your star schema. Weâ€™ll create a schema.yml file in the models/marts/ folder that validates primary keys, non-null columns, and optionally unique constraints. We'll to include relationships/foreign keys so dbt can build lineage and document your star schema fully.

ðŸ”¹ models/marts/schema.yml
```
version: 2

models:
  - name: dim_customers
    description: "Dimension table for customers"
    columns:
      - name: customer_id
        description: "Primary key for customer"
        tests:
          - not_null
          - unique
      - name: customer_unique_id
        tests:
          - not_null
      - name: customer_city
      - name: customer_state

  - name: dim_products
    description: "Dimension table for products"
    columns:
      - name: product_id
        description: "Primary key for product"
        tests:
          - not_null
          - unique
      - name: product_category_name_english
      - name: product_weight_g
      - name: product_length_cm
      - name: product_height_cm
      - name: product_width_cm

  - name: dim_sellers
    description: "Dimension table for sellers"
    columns:
      - name: seller_id
        description: "Primary key for seller"
        tests:
          - not_null
          - unique
      - name: seller_city
      - name: seller_state

  - name: dim_dates
    description: "Dimension table for dates"
    columns:
      - name: date_key
        description: "Primary key for date"
        tests:
          - not_null
          - unique
      - name: full_date
      - name: day_of_week
      - name: month_name
      - name: year
      - name: quarter

  - name: dim_payments
    description: "Dimension table for payments"
    columns:
      - name: payment_type_key
        description: "Primary key for payment type"
        tests:
          - not_null
          - unique
      - name: payment_type

  - name: fact_order_items
    description: "Fact table for order items"
    columns:
      - name: order_id
        description: "Primary key for order item"
        tests:
          - not_null
          - unique
        relationships:
          - to: ref('dim_customers')
            field: customer_id
      - name: order_item_id
        tests:
          - not_null
      - name: product_id
        tests:
          - not_null
        relationships:
          - to: ref('dim_products')
            field: product_id
      - name: seller_id
        tests:
          - not_null
        relationships:
          - to: ref('dim_sellers')
            field: seller_id
      - name: customer_id
        tests:
          - not_null
        relationships:
          - to: ref('dim_customers')
            field: customer_id
      - name: order_date_key
        tests:
          - not_null
        relationships:
          - to: ref('dim_dates')
            field: date_key
      - name: payment_type_key
        tests:
          - not_null
        relationships:
          - to: ref('dim_payments')
            field: payment_type_key
      - name: price
      - name: freight_value
      - name: review_score
      - name: delivery_time_days
```

ðŸ”¹ How to run dbt tests

After building models:
```
dbt test
```
This will validate:

	â€¢	All primary key columns are unique and not null
	â€¢	Other critical columns (customer_id, product_id, etc.) are not null

â¸»

âœ… Result: You now have a fully validated star schema with:

	â€¢	Staging models (stg_*)
	â€¢	Dimension tables (dim_*)
	â€¢	Incremental fact table (fact_order_items)
	â€¢	dbt tests to catch schema or missing data issues

## 8. Documentation

1.	Foreign keys: Each column in fact_order_items references the corresponding primary key in the dimension table.
2.	dbt documentation: When you run:
```
dbt docs generate
dbt docs serve
```
Youâ€™ll see the full lineage graph showing all fact â†’ dimension relationships.

3. Tests: dbt will also validate that the relationships exist, so missing dimension references are caught if you add relationships tests.

âœ… Result: Your project now has:

	â€¢	Full star schema
	â€¢	Primary key / uniqueness / not null tests
	â€¢	Foreign key relationships for lineage and validation
	â€¢	Incremental fact table to reduce cost on BigQuery
	â€¢	If using BigQuery: Youâ€™ll first need to stage the CSVs into a dataset (raw) via Meltano or bq load.

## How it flows
	â€¢	dbt will read from m2_ingestion.* (sources).
	â€¢	dbt will write models into m2_prod.*.
	â€¢	Your fact + dim tables will land in m2_prod dataset.

## 9. dbt Sequence Run
Hereâ€™s a ready-to-run dbt execution sequence for your BigQuery setup with seeds, staging, and incremental fact table. This ensures everything loads in the correct order and leverages incremental processing for the fact table.

### Step 1: Validate Connection
```
dbt debug
```

	â€¢	Checks your profiles.yml for BigQuery credentials and connectivity.
	â€¢	Make sure your target dataset (m2_prod) exists or dbt can create tables there.

### Step 2: Load Seeds
```
dbt seed
```

	â€¢	Loads your product_category_name_translation.csv into BigQuery.
	â€¢	Seed tables can be referenced in dim_products for English category names.

### Step 3: Build Staging Views
```
dbt run --select staging
```

	â€¢	Builds all stg_* views in BigQuery.
	â€¢	These are lightweight and ensure all raw data is cleaned/formatted for marts.

### Step 4: Build Dimensions
```
dbt run --select marts.dim_*
```

	â€¢	Creates all dim_* tables (dim_customers, dim_products, dim_sellers, dim_dates, dim_payments) in m2_prod.
	â€¢	Dimensions are fully rebuilt every run (small tables, inexpensive).

### Step 5: Build Incremental Fact Table
```
dbt run --select marts.fact_order_items
```

	â€¢	Builds fact_order_items using incremental merge strategy.
	â€¢	Only new or updated orders are appended.
	â€¢	unique_key=order_id ensures deduplication.

### Step 6: Run Tests
```
dbt test
```

	â€¢	Validates primary keys, not null columns, unique constraints, and foreign key relationships.
	â€¢	Ensures integrity between fact and dimension tables.

### Step 7: Generate Documentation
```
dbt docs generate
dbt docs serve
```

	â€¢	Opens a browser with full lineage and documentation for your star schema.
	â€¢	Relationships between fact and dimensions are visible in the graph.

ðŸ”¹ Notes / Best Practices

	â€¢	Incremental fact table: you can rerun dbt run --select fact_order_items daily/weekly as new orders arrive without reprocessing the entire dataset.
	â€¢	Dimension rebuild: cheap to fully refresh each run.
	â€¢	Seeds: any reference tables (like product_category_name_translation) are loaded first.
	â€¢	Testing: dbt test should be run after every full load or before production refresh.
