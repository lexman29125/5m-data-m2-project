{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d606a6a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282dd157",
   "metadata": {},
   "source": [
    "End to end Data Pipeline for M2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21292f4",
   "metadata": {},
   "source": [
    "# Ingest Brazilian E-Commerce Public Dataset by Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d03fb",
   "metadata": {},
   "source": [
    "Option A: Run command in terminal: curl -L -o \"/Users/alexfoo/Documents/NTU_DSAI/5m-data-m2-project/assets/brazilian-ecommerce.zip\" https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbf448",
   "metadata": {},
   "source": [
    "Option B: Run in cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6bfa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded zip to: /Users/alexfoo/Documents/NTU_DSAI/5m-data-m2-project/assets/brazilian-ecommerce.zip\n",
      "Files extracted to: /Users/alexfoo/Documents/NTU_DSAI/5m-data-m2-project/assets/\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, os\n",
    "\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce\"\n",
    "save_path = \"/Users/alexfoo/Documents/NTU_DSAI/5m-data-m2-project/assets/\"\n",
    "zip_file = os.path.join(save_path, \"brazilian-ecommerce.zip\")\n",
    "\n",
    "# Download file\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_file, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Downloaded zip to: {zip_file}\")\n",
    "else:\n",
    "    print(\"Download failed:\", response.status_code)\n",
    "\n",
    "# Unzip\n",
    "with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(save_path)\n",
    "\n",
    "print(f\"Files extracted to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96b601",
   "metadata": {},
   "source": [
    "# Create a Meltano Project\n",
    "We will create a Meltano project and use it to\n",
    "extract data from CSVs and load it into a BigQuery dataset.\n",
    "\n",
    "We will treat the BigQuery dataset as our data warehouse. The 2 tasks above are typical data ingestion pipelines, which extract data from external and internal sources and load them into a data warehouse.\n",
    "\n",
    "To create a Meltano project, run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e301f",
   "metadata": {},
   "source": [
    "To create a Meltano project, run:\n",
    "\n",
    "meltano init meltano-ingestion\n",
    "cd meltano-ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5be7d",
   "metadata": {},
   "source": [
    "# Add an Extractor to Pull Data from CSVs\n",
    "We're going to add an extrator for CSV to get our data. An extractor is responsible for pulling data out of any data source. We will use the tap-csv extractor to pull the dataset from downloaded CSVs.\n",
    "\n",
    "To add the extractor to our project, run:\n",
    "\n",
    "meltano add extractor tap-csv\n",
    "\n",
    "Next, configure the extractor by running:\n",
    "\n",
    "meltano config tap-csv set --interactive\n",
    "\n",
    "You will be prompted to enter many options, we just need to enter the following:\n",
    "\n",
    "Settings\n",
    "| 1. add_metadata_columns: When True, add the metadata columns (`_sdc_source_file`, `_sdc_source_fi...\n",
    "│ 2. csv_files_definition: Project-relative path to JSON file holding array of objects as described...\n",
    "│ 3. faker_config.locale: One or more LCID locale strings to produce localized output for: https:/...\n",
    "│ 4. faker_config.seed: Value to seed the Faker generator for deterministic output: https://fake...\n",
    "│ 5. files: Array of objects with `entity`, `path`, `keys`, and `encoding` [Optional...\n",
    "│ 6. flattening_enabled: 'True' to enable schema flattening and automatically expand nested prope... \n",
    "│ 7. flattening_max_depth: The max depth to flatten schemas.\n",
    "│ 8. stream_map_config: User-defined config values to be used within map expressions.\n",
    "│ 9. stream_maps: Config object for stream maps capability. For more information check out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da9c8c0",
   "metadata": {},
   "source": [
    "You will be prompted to enter many options, we just need to enter the following:\n",
    "\n",
    "flattening_enabled: true\n",
    "flattening_max_depth: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333d49b",
   "metadata": {},
   "source": [
    "In Meltano, when you use tap-csv, you configure it to point at a directory of CSV files. The tap can then ingest multiple CSVs in one go, with each CSV treated as a stream.\n",
    "\n",
    "Configure it in meltano.yml\n",
    "\n",
    "Example config (simplified):\n",
    "\n",
    "plugins:\n",
    "  extractors:\n",
    "  - name: tap-csv\n",
    "    variant: meltanolabs\n",
    "    pip_url: git+https://github.com/MeltanoLabs/tap-csv.git\n",
    "    config:\n",
    "      files:\n",
    "        - entity: customer\n",
    "          path: ../assets/olist_customers_dataset.csv\n",
    "          keys: [customer_id]\n",
    "        - entity: geolocation\n",
    "          path: ../assets/olist_geolocation_dataset.csv\n",
    "          keys: [geolocation_zip_code_prefix]\n",
    "        - entity: order_item\n",
    "          path: ../assets/olist_order_items_dataset.csv\n",
    "          keys: [order_item_id]\n",
    "        - entity: order_payment\n",
    "          path: ../assets/olist_order_payments_dataset.csv\n",
    "          keys: [order_payment_id]          \n",
    "        - entity: order_review\n",
    "          path: ../assets/olist_order_reviews_dataset.csv\n",
    "          keys: [order_reviews_id]   \n",
    "        - entity: order\n",
    "          path: ../assets/olist_orders_dataset.csv\n",
    "          keys: [order_id]   \n",
    "        - entity: product\n",
    "          path: ../assets/olist_products_dataset.csv\n",
    "          keys: [product_id]   \n",
    "        - entity: seller\n",
    "          path: ../assets/olist_sellers_dataset.csv\n",
    "          keys: [seller_id]   \n",
    "        - entity: product_category_name_translation\n",
    "          path: ../assets/product_category_name_translation.csv\n",
    "          keys: [product_category_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82c4ea",
   "metadata": {},
   "source": [
    "Test that extractor settings are valid using meltano config :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74244e2b",
   "metadata": {},
   "source": [
    "meltano config tap-csv test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887db8bc",
   "metadata": {},
   "source": [
    "# Add a Loader to Load Data into BigQuery\n",
    "\n",
    "Before we can load the data into BigQuery, let's create a new project called meltano-learn. Then create a dataset in BigQuery called m2_ingestion (multi-region: US).\n",
    "\n",
    "Finally, create a service account with the BigQuery Admin role and download the JSON key file to your local machine.\n",
    "\n",
    "We will now add a loader to load the data into BigQuery.\n",
    "\n",
    "meltano add loader target-bigquery\n",
    "\n",
    "meltano config target-bigquery set --interactive\n",
    "\n",
    "Set the following options:\n",
    "\n",
    "project: <GCP Project ID>\n",
    "dataset: m2_ingestion\n",
    "credentials_path: full path to the service account key file\n",
    "method: batch_job\n",
    "denormalized: true\n",
    "flattening_enabled: true\n",
    "flattening_max_depth: 1\n",
    "batch_size: 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6ee9c",
   "metadata": {},
   "source": [
    "We can now run the full ingestion (extract-load) pipeline.\n",
    "\n",
    "meltano run tap-csv target-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576048e4",
   "metadata": {},
   "source": [
    "# Job Failed: BrokenPipeError: [Errno 32] Broken pipe\n",
    "\n",
    "Option A — Increase the file descriptor limit\n",
    "Temporarily raise limit (for current shell session)\n",
    "ulimit -n 4096\n",
    "\n",
    "Option B — Batch your CSVs\n",
    "\t•\tInstead of reading hundreds of files at once, process them in smaller groups.\n",
    "\t•\tIn meltano.yml, only list a subset of CSVs as streams per run.\n",
    "\n",
    "Option C - Provision a GCP VM with more I/O to run job\n",
    "\n",
    "Option D - Reduce batch size\n",
    "\t•\tIn your meltano.yml or tap configuration, reduce how many records are buffered before flushing.\n",
    "\n",
    "    config:\n",
    "  batch_size: 500  # default might be 1000+\n",
    "\n",
    "Combination of options A, B and D, the local machine is able to complete the jobs successfully with ~10mins per batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c40c7",
   "metadata": {},
   "source": [
    "# Job Failed 2:\n",
    "\n",
    "When running product_category_name_translation.csv\n",
    "\n",
    "[info     ] google.api_core.exceptions.BadRequest: 400 POST https://bigquery.googleapis.com/bigquery/v2/projects/sound-vehicle-468314-q4/datasets/m2_ingestion/tables?prettyPrint=false: Invalid field name \"﻿product_category_name\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675830e",
   "metadata": {},
   "source": [
    "# Possible Solution 2 (Not working):\n",
    "\n",
    "In meltano.yml for tap-csv, add:\n",
    "\n",
    "plugins:\n",
    "  extractors:\n",
    "    - name: tap-csv\n",
    "      config:\n",
    "        files:\n",
    "          - entity: product_category_name_translation\n",
    "            path: ./data/product_category_name_translation.csv\n",
    "            keys: [product_category_name]\n",
    "            clean_headers: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072820d9",
   "metadata": {},
   "source": [
    "\n",
    "# Fix:\n",
    "UTF-8 BOM issue in your CSV header. The fix is to physically strip the BOM from the file before tap-csv hands it off to target-bigquery.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"product_category_name_translation.csv\", encoding=\"utf-8-sig\")\n",
    "df.to_csv(\"product_category_name_translation.csv\", index=False)\n",
    "\n",
    "or\n",
    "\n",
    "sed -i '1s/^\\xEF\\xBB\\xBF//' product_category_name_translation.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
